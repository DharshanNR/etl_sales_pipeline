# E-commerce Data ETL Pipeline with PySpark

This repository contains a comprehensive ETL (Extract, Transform, Load) pipeline built with PySpark for processing synthetic e-commerce sales data. The pipeline covers data loading, cleaning, transformation, aggregation, and storage, providing a robust framework for data analysis.

## Table of Contents

1.  [Project Overview](#project-overview)
2.  [Data Generation](#data-generation)
3.  [ETL Pipeline (`etl_pipeline.py`)](#3-etl-pipeline-etl_pipelinepy)
    *   [Features](#features)
    *   [Data Sources](#data-sources)
    *   [Output](#output)
4.  [Setup and Installation](#setup-and-installation)
5.  [How to Run](#how-to-run)
6.  [Code Structure](#code-structure)
7.  [Dependencies](#dependencies)
8.  [License](#license)

## 1. Project Overview

This project demonstrates an end-to-end data processing workflow for e-commerce sales data using Apache Spark with its Python API, PySpark. It's designed to take raw CSV data, clean and transform it, derive meaningful insights through aggregations and advanced analytics (like window functions), and finally store the processed data in CSV file for downstream consumption.

## 2. Data Generation

Before running the ETL pipeline, you need to generate the synthetic e-commerce sales data. This is done using a separate Python script (provided in the previous response, let's call it `generate_data.py`).

The `generate_data.py` script creates the following CSV files:

*   `customers.csv`: Contains customer details.
*   `products.csv`: Contains product information.
*   `orders.csv`: Contains order details.
*   `order_items.csv`: Contains details of items within each order.

**To generate the data:**

1.  Ensure you have `Faker` and `pandas` installed:
    ```bash
    pip install Faker pandas
    ```
2.  Run the `generate_data.py` script:
    ```bash
    python generate_data.py
    ```
    This will create the necessary CSV files in the current directory.

### 3. ETL Pipeline (`etl_pipeline.py`)

The `etl_pipeline.py` script is the core of this project. It performs the following steps:

### Features

*   **Data Loading:** Reads raw CSV files into PySpark DataFrames.
*   **Schema Definition:** Explicitly defines schemas for all input DataFrames for better control, performance, and error handling.
*   **Data Cleaning:**
    *   Handles missing values (e.g., dropping rows with critical nulls).
    *   Converts data types to appropriate formats (e.g., `order_date` to `TimestampType`).
*   **Data Transformation:**
    *   **Feature Engineering:** Calculates `item_total` (quantity * price_per_unit) and extracts `year`, `month`, `day_of_week` from `order_date`.
    *   **Data Joining:** Joins `orders`, `order_items`, `products`, and `customers` DataFrames to create a comprehensive, denormalized "fact table".
    *   **Data Filtering:** Filters out cancelled orders and orders with invalid total amounts.
    *   **Data Standardization:** Converts text fields to lowercase and trims whitespace for consistency.
*   **Data Aggregation & Analysis:**
    *   Calculates various Key Performance Indicators (KPIs):
        *   Total Sales by Date
        *   Total Sales by Product Category
        *   Top N Products/Customers by Sales
        *   Monthly/Quarterly Sales Trends
        *   Average Order Value (AOV)
        *   Sales by Country
    *   **Advanced Analytics (Window Functions):**
        *   Running totals of sales.
        *   Ranking products within categories by sales.
        *   Month-over-month sales growth.
*   **Data Storage:**
    *   Stores the cleaned and transformed fact table in Parquet format.
    *   Stores the fact table partitioned by `order_year` and `order_month` for optimized query performance.
    *   Stores all generated aggregated results (KPIs) in Parquet format.

### Data Sources

The pipeline expects the following CSV files (generated by `generate_data.py`) to be present in the `input_data_path` (default: `./`):

*   `customers.csv`
*   `products.csv`
*   `orders.csv`
*   `order_items.csv`

### Output

The processed data and aggregated results are stored in Parquet format in the `output_data_path` (default: `./output_data/`). The output directory will contain:

*   `fact_table.parquet`: The main denormalized fact table.
*   `fact_table_partitioned.parquet`: The fact table partitioned by `order_year` and `order_month`.
*   Individual Parquet files for each aggregated result (e.g., `sales_by_date.parquet`, `top_n_products.parquet`, etc.).

## 4. Setup and Installation

1.  **Apache Spark:** Ensure you have Apache Spark installed and configured on your system. You can download it from the [official Apache Spark website](https://spark.apache.org/downloads.html).
2.  **PySpark:** Install PySpark using pip:
    ```bash
    pip install pyspark pandas Faker
    ```
    (Pandas and Faker are needed for the data generation script).

## 5. How to Run

1.  **Generate Data:** First, run the `generate_data.py` script to create the input CSV files:
    ```bash
    python generate_data.py
    ```
    This will create `customers.csv`, `products.csv`, `orders.csv`, and `order_items.csv` in your current directory.

2.  **Run ETL Pipeline:** Execute the `etl_pipeline.py` script using `spark-submit`:
    ```bash
    spark-submit etl_pipeline.py
    ```

    The script will print progress messages to the console. Upon successful completion, the processed data will be available in the `./output_data/` directory.

## 6. Code Structure

*   `etl_pipeline.py`:
    *   `initialize_spark_session()`: Sets up the Spark environment.
    *   `load_data()`: Reads CSVs and performs initial exploration.
    *   `clean_and_transform_data()`: Handles data cleaning, type conversions, feature engineering, and joins.
    *   `perform_analysis()`: Calculates KPIs and advanced analytics.
    *   `store_processed_data()`: Saves results to Parquet.
    *   `if __name__ == "__main__":`: Orchestrates the entire ETL workflow.

## 7. Dependencies

*   `pyspark`: For Spark DataFrame operations.
*   `pandas`: (Used by `generate_data.py`) For creating initial dataframes.
*   `Faker`: (Used by `generate_data.py`) For generating synthetic data.

## 8. License

This project is open-source and available under the [MIT License](LICENSE).
